# Integraci√≥n con Qdrant DB

## Descripci√≥n

La aplicaci√≥n ahora incluye integraci√≥n con Qdrant Cloud para b√∫squeda vectorial de contexto relevante. Esta funcionalidad **solo se activa cuando OpenAI est√° configurado**, ya que utiliza los embeddings de OpenAI para realizar b√∫squedas sem√°nticas en la base de datos vectorial.

## Requisitos

1. **OpenAI API Key**: Necesaria para generar embeddings y usar el chat con IA
2. **Qdrant Cloud**: Cluster en la nube con una colecci√≥n de datos clim√°ticos/agr√≠colas
3. **Qdrant API Key**: Credenciales de acceso a tu cluster

## Configuraci√≥n

### 1. Variables de Entorno

#### Opci√≥n A: Desarrollo Local (archivo .env)

Crea un archivo `.env` en la ra√≠z del proyecto con las siguientes variables:

```bash
# OpenAI (obligatorio para usar Qdrant)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini

# Qdrant Cloud (opcional)
QDRANT_URL=https://tu-cluster.qdrant.io:6333
QDRANT_API_KEY=tu_api_key_aqui

# Modelo de embeddings (debe coincidir con el usado en Qdrant)
EMBEDDING_MODEL=text-embedding-3-large
```

**Importante**: El `EMBEDDING_MODEL` debe ser el mismo que usaste para crear los vectores en Qdrant:
- `text-embedding-3-small` ‚Üí 1536 dimensiones
- `text-embedding-3-large` ‚Üí 3072 dimensiones (por defecto)

Si usas archivo `.env`, instala tambi√©n:
```bash
pip install python-dotenv
```

#### Opci√≥n B: Hugging Face Spaces

En Hugging Face Spaces, configura los secretos en la configuraci√≥n del Space:
- `OPENAI_API_KEY`
- `OPENAI_MODEL` (opcional, por defecto: gpt-4o-mini)
- `QDRANT_URL`
- `QDRANT_API_KEY`
- `EMBEDDING_MODEL` (opcional, por defecto: text-embedding-3-large)

Las variables ya estar√°n disponibles autom√°ticamente, no necesitas `python-dotenv`.

### 2. Instalaci√≥n de Dependencias

```bash
pip install -r requirements.txt
```

Esto instalar√°:
- `qdrant-client>=1.7.0` - Cliente de Qdrant

**Nota**: `python-dotenv` es opcional y solo necesario para desarrollo local con archivo `.env`.

### 3. Carga de Variables

El archivo `app.py` detecta autom√°ticamente el entorno:

```python
# Intenta cargar .env si est√° disponible (desarrollo local)
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # Si no est√° instalado, usa variables del sistema (Hugging Face Spaces)
    pass
```

Esto permite que la aplicaci√≥n funcione en ambos entornos sin modificaciones.

## Funcionamiento

### Flujo de B√∫squeda

1. **Detecci√≥n de OpenAI**: Al iniciar el chat, el sistema verifica si OpenAI est√° activo
2. **Inicializaci√≥n de Qdrant**: Si OpenAI est√° presente, se intenta conectar a Qdrant
3. **B√∫squeda Vectorial**: Cuando el usuario env√≠a un mensaje:
   - Se genera un embedding del mensaje usando OpenAI (modelo configurable, por defecto `text-embedding-3-large`)
   - Se buscan los 3 documentos m√°s relevantes en Qdrant
   - El contexto se inyecta en el prompt del LLM
4. **Respuesta Enriquecida**: El modelo responde usando tanto el reporte clim√°tico como el contexto de Qdrant

### Estructura de la Colecci√≥n

La colecci√≥n en Qdrant debe tener la siguiente estructura:

```python
{
    "text": "Contenido del documento",
    "metadata": {
        "source": "fuente_del_documento",
        "tipo": "recomendacion_riego|info_climatica|etc",
        # ... otros metadatos
    }
}
```

## Comportamiento sin Qdrant

Si Qdrant no est√° configurado o no est√° disponible:

- ‚úÖ La aplicaci√≥n funciona normalmente
- ‚úÖ El chat usa solo el reporte clim√°tico generado
- ‚ö†Ô∏è No se obtiene contexto adicional de la base de conocimientos
- üìù Se muestra un mensaje en consola: "‚ö†Ô∏è Qdrant no disponible"

## Comportamiento sin OpenAI

Si OpenAI no est√° configurado:

- ‚úÖ La aplicaci√≥n funciona con Hugging Face
- ‚ùå Qdrant NO se inicializa (requiere OpenAI para embeddings)
- ‚úÖ El chat funciona con modelos de Hugging Face

## Logs y Debugging

El sistema imprime mensajes informativos en consola:

```
‚úÖ Qdrant conectado y disponible para b√∫squeda vectorial
üìö Contexto de Qdrant obtenido (1234 caracteres)
‚ö†Ô∏è Qdrant no disponible (verifica QDRANT_URL y QDRANT_API_KEY)
‚ö†Ô∏è Error buscando en Qdrant: [mensaje de error]
```

## C√≥digo Relevante

### Archivos Modificados

1. **`mcp_core.py`**: 
   - Clase `QdrantRetriever` para b√∫squeda vectorial
   - M√©todo `is_openai_active()` en clase `LLM`

2. **`ui.py`**:
   - Funci√≥n `_chat_respond` actualizada para usar Qdrant
   - Estado `qdrant_state` para mantener la conexi√≥n

3. **`agents/advisor_agent.py`**:
   - M√©todo `_build_prompt` actualizado para incluir contexto de Qdrant

4. **`requirements.txt`**:
   - Dependencia `qdrant-client>=1.7.0`

## Ejemplo de Uso

```python
# En tu c√≥digo Python
from mcp_core import QdrantRetriever

# Inicializar (requiere variables de entorno)
retriever = QdrantRetriever(collection_name="recomendador-de-riego")

# Verificar disponibilidad
if retriever.is_available():
    # Buscar contexto
    context = retriever.get_context("¬øC√≥mo regar en √©poca de sequ√≠a?", limit=3)
    print(context)
```

## Troubleshooting

### Error: "No se pudo conectar a Qdrant"

- Verifica que `QDRANT_URL` y `QDRANT_API_KEY` est√©n correctamente configuradas
- Aseg√∫rate de que tu cluster de Qdrant est√© activo
- Verifica la conectividad de red

### Error: "Error generando embedding"

- Verifica que `OPENAI_API_KEY` sea v√°lida
- Revisa tu cuota de API de OpenAI
- Aseg√∫rate de tener cr√©ditos disponibles

### Qdrant no se inicializa

- Verifica que OpenAI est√© configurado primero
- Revisa los logs en consola para mensajes de error
- Aseg√∫rate de que `qdrant-client` est√© instalado

## Seguridad

- ‚ö†Ô∏è **Nunca** commitees el archivo `.env` al repositorio
- ‚úÖ Usa `.env.example` como plantilla
- ‚úÖ Mant√©n tus API keys seguras y rotadas regularmente
- ‚úÖ Usa variables de entorno en producci√≥n
